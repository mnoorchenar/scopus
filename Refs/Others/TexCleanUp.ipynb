{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbd0f46e",
   "metadata": {},
   "source": [
    "# Overleaf to db file\n",
    "input: main.tex Refs.bib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "075425ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def parse_citations_from_tex(tex_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse a LaTeX .tex file to extract citations, their frequency,\n",
    "    and the section(s) they appear in.\n",
    "\n",
    "    Parameters:\n",
    "        tex_file (str): Path to the .tex file\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A table with columns [Reference, Frequency, Sections]\n",
    "    \"\"\"\n",
    "    # Read file and ignore comment lines\n",
    "    with open(tex_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Remove commented lines (starting with %)\n",
    "    clean_text = \"\\n\".join(line for line in lines if not line.strip().startswith(\"%\"))\n",
    "\n",
    "    # Regex patterns\n",
    "    section_pattern = re.compile(r'\\\\section\\{([^}]*)\\}(?:\\\\label\\{[^}]*\\})?')\n",
    "    cite_pattern = re.compile(r'\\\\cite\\{([^}]*)\\}')\n",
    "\n",
    "    # Split into sections\n",
    "    sections = section_pattern.split(clean_text)\n",
    "\n",
    "    citations = []\n",
    "    ref_sections = {}\n",
    "\n",
    "    # Iterate through sections\n",
    "    for i in range(1, len(sections), 2):\n",
    "        section_name = sections[i].strip()\n",
    "        section_text = sections[i+1]\n",
    "\n",
    "        # Find citations in this section\n",
    "        matches = cite_pattern.findall(section_text)\n",
    "        for match in matches:\n",
    "            for key in match.split(\",\"):\n",
    "                ref = key.strip()\n",
    "                citations.append(ref)\n",
    "                if ref not in ref_sections:\n",
    "                    ref_sections[ref] = []\n",
    "                if section_name not in ref_sections[ref]:\n",
    "                    ref_sections[ref].append(section_name)\n",
    "\n",
    "    # Count frequencies while preserving order of first appearance\n",
    "    freq = {}\n",
    "    order = []\n",
    "    for c in citations:\n",
    "        if c not in freq:\n",
    "            order.append(c)\n",
    "        freq[c] = freq.get(c, 0) + 1\n",
    "\n",
    "    # Build DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"Reference\": order,\n",
    "        \"Frequency\": [freq[c] for c in order],\n",
    "        \"Sections\": [\", \".join(ref_sections[c]) for c in order]\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def parse_bibtex_to_dataframe(bib_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse a .bib file and return a DataFrame with structured fields\n",
    "    plus the full BibTeX entry.\n",
    "\n",
    "    Parameters:\n",
    "        bib_file (str): Path to the .bib file\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Parsed bibliography\n",
    "    \"\"\"\n",
    "    # Read the file\n",
    "    with open(bib_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Split into entries\n",
    "    entries = [\"@\" + e for e in content.split(\"@\") if e.strip()]\n",
    "\n",
    "    papers = []\n",
    "\n",
    "    for entry in entries:\n",
    "        # Extract type and key\n",
    "        match = re.match(r'@(\\w+)\\s*\\{([^,]+),', entry)\n",
    "        if not match:\n",
    "            continue\n",
    "        entry_type, entry_key = match.groups()\n",
    "\n",
    "        # Extract fields (like title, author, year, etc.)\n",
    "        fields = dict(re.findall(\n",
    "            r'(\\w+)\\s*=\\s*\\{((?:[^{}]|\\{[^}]*\\})*)\\}', \n",
    "            entry, \n",
    "            flags=re.DOTALL\n",
    "        ))\n",
    "\n",
    "        # Normalize important fields\n",
    "        authors = fields.get(\"author\", \"\").strip()\n",
    "        title = fields.get(\"title\", \"\").strip()\n",
    "        journal = fields.get(\"journal\", fields.get(\"booktitle\", \"\")).strip()\n",
    "        year = fields.get(\"year\", \"\").strip()\n",
    "        publisher = fields.get(\"publisher\", fields.get(\"organization\", \"\")).strip()\n",
    "\n",
    "        papers.append({\n",
    "            \"Key\": entry_key,\n",
    "            \"Type\": entry_type,\n",
    "            \"Authors\": authors,   # üëà NEW column for full author names\n",
    "            \"Title\": title,\n",
    "            \"Journal/Booktitle\": journal,\n",
    "            \"Year\": year,\n",
    "            \"Publisher\": publisher,\n",
    "            \"BibTeX\": entry.strip()  # full raw entry\n",
    "        })\n",
    "\n",
    "    # Build DataFrame, keeping unique keys\n",
    "    df = pd.DataFrame(papers).drop_duplicates(subset=\"Key\", keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_citations_with_bib(main_text_df: pd.DataFrame, references_bib_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge LaTeX citations (from main.tex) with BibTeX records (from references.bib),\n",
    "    keeping the same order as in the LaTeX file.\n",
    "    \"\"\"\n",
    "    # Convert BibTeX records into a lookup dictionary by Key\n",
    "    bib_lookup = references_bib_df.set_index(\"Key\").to_dict(orient=\"index\")\n",
    "\n",
    "    merged_records = []\n",
    "    for _, row in main_text_df.iterrows():\n",
    "        key = row[\"Reference\"]\n",
    "        bib_info = bib_lookup.get(key, {})  # safely lookup\n",
    "        merged_records.append({\n",
    "            \"Reference\": key,\n",
    "            \"Frequency\": row[\"Frequency\"],\n",
    "            \"Sections\": row[\"Sections\"],\n",
    "            \"Type\": bib_info.get(\"Type\", \"\"),\n",
    "            \"Authors\": bib_info.get(\"Authors\", \"\"),   # üëà ADD AUTHORS HERE\n",
    "            \"Title\": bib_info.get(\"Title\", \"\"),\n",
    "            \"Journal/Booktitle\": bib_info.get(\"Journal/Booktitle\", \"\"),\n",
    "            \"Year\": bib_info.get(\"Year\", \"\"),\n",
    "            \"Publisher\": bib_info.get(\"Publisher\", \"\"),\n",
    "            \"BibTeX\": bib_info.get(\"BibTeX\", \"\")\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(merged_records)\n",
    "\n",
    "\n",
    "references_bib = parse_bibtex_to_dataframe(\"references.bib\")\n",
    "main_text = parse_citations_from_tex(\"main.tex\")\n",
    "\n",
    "merged_df = merge_citations_with_bib(main_text, references_bib)\n",
    "# Add index column starting from 1\n",
    "merged_df.insert(0, \"Index\", range(1, len(merged_df) + 1))\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "# Save merged_df to SQLite\n",
    "conn = sqlite3.connect(\"references.db\")\n",
    "merged_df.to_sql(\"references\", conn, if_exists=\"replace\", index=False)\n",
    "conn.commit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55539d",
   "metadata": {},
   "source": [
    "# Cross Ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ac11b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/30] Processing Reference=chen2020renewable\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=390\n",
      "‚è≥ Waiting 10.3 seconds before next request...\n",
      "\n",
      "[2/30] Processing Reference=somu2021deep\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=406\n",
      "‚è≥ Waiting 13.4 seconds before next request...\n",
      "\n",
      "[3/30] Processing Reference=hassan2016systematic\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=517\n",
      "‚è≥ Waiting 7.2 seconds before next request...\n",
      "\n",
      "[4/30] Processing Reference=lin2022hybrid\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=482\n",
      "‚è≥ Waiting 8.3 seconds before next request...\n",
      "\n",
      "[5/30] Processing Reference=10734172\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=484\n",
      "‚è≥ Waiting 9.1 seconds before next request...\n",
      "\n",
      "[6/30] Processing Reference=al2023hybrid\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=391\n",
      "‚è≥ Waiting 14.1 seconds before next request...\n",
      "\n",
      "[7/30] Processing Reference=kim2023novel\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=393\n",
      "‚è≥ Waiting 13.6 seconds before next request...\n",
      "\n",
      "[8/30] Processing Reference=fekri2021deep\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=437\n",
      "‚è≥ Waiting 10.0 seconds before next request...\n",
      "\n",
      "[9/30] Processing Reference=noorchenarboo2025explaining\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=435\n",
      "‚è≥ Waiting 8.9 seconds before next request...\n",
      "\n",
      "[10/30] Processing Reference=antwarg2021explaining\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=429\n",
      "‚è≥ Waiting 10.9 seconds before next request...\n",
      "\n",
      "[11/30] Processing Reference=chen2023algorithms\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=428\n",
      "‚è≥ Waiting 8.2 seconds before next request...\n",
      "\n",
      "[12/30] Processing Reference=dehrouyeh2024tinyml\n",
      "‚úÖ Updated: Similarity=0.99, BibTeX length=453\n",
      "‚è≥ Waiting 8.9 seconds before next request...\n",
      "\n",
      "[13/30] Processing Reference=aras2022interpretable\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=368\n",
      "‚è≥ Waiting 7.0 seconds before next request...\n",
      "\n",
      "[14/30] Processing Reference=lundberg2017unified\n",
      "‚úÖ Updated: Similarity=0.41, BibTeX length=425\n",
      "‚è≥ Waiting 14.1 seconds before next request...\n",
      "\n",
      "[15/30] Processing Reference=dehrouyeh2025pruning\n",
      "‚úÖ Updated: Similarity=0.61, BibTeX length=323\n",
      "‚è≥ Waiting 11.9 seconds before next request...\n",
      "\n",
      "[16/30] Processing Reference=van2024harnessing\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=455\n",
      "‚è≥ Waiting 5.1 seconds before next request...\n",
      "\n",
      "[17/30] Processing Reference=jethani2021fastshap\n",
      "‚úÖ Updated: Similarity=0.68, BibTeX length=288\n",
      "‚è≥ Waiting 9.1 seconds before next request...\n",
      "\n",
      "[18/30] Processing Reference=gao2021interpretable\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=393\n",
      "‚è≥ Waiting 9.5 seconds before next request...\n",
      "\n",
      "[19/30] Processing Reference=ozcan2021energy\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=363\n",
      "‚è≥ Waiting 6.1 seconds before next request...\n",
      "\n",
      "[20/30] Processing Reference=gonccalves2023variable\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=438\n",
      "‚è≥ Waiting 10.4 seconds before next request...\n",
      "\n",
      "[21/30] Processing Reference=han2024short\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=429\n",
      "‚è≥ Waiting 6.8 seconds before next request...\n",
      "\n",
      "[22/30] Processing Reference=peng2024explainable\n",
      "‚úÖ Updated: Similarity=0.99, BibTeX length=458\n",
      "‚è≥ Waiting 9.3 seconds before next request...\n",
      "\n",
      "[23/30] Processing Reference=choi2022explainable\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=462\n",
      "‚è≥ Waiting 13.9 seconds before next request...\n",
      "\n",
      "[24/30] Processing Reference=lu2024incorporating\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=458\n",
      "‚è≥ Waiting 9.2 seconds before next request...\n",
      "\n",
      "[25/30] Processing Reference=wang2024shap\n",
      "‚úÖ Updated: Similarity=0.99, BibTeX length=565\n",
      "‚è≥ Waiting 14.0 seconds before next request...\n",
      "\n",
      "[26/30] Processing Reference=zhou2024deciphering\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=409\n",
      "‚è≥ Waiting 14.3 seconds before next request...\n",
      "\n",
      "[27/30] Processing Reference=amiri2023investigating\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=511\n",
      "‚è≥ Waiting 8.0 seconds before next request...\n",
      "\n",
      "[28/30] Processing Reference=aas2021explaining\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=422\n",
      "‚è≥ Waiting 13.7 seconds before next request...\n",
      "\n",
      "[29/30] Processing Reference=weather_canada_historical_data\n",
      "‚úÖ Updated: Similarity=0.65, BibTeX length=206\n",
      "‚è≥ Waiting 13.3 seconds before next request...\n",
      "\n",
      "[30/30] Processing Reference=Miller2020yc\n",
      "‚úÖ Updated: Similarity=1.00, BibTeX length=590\n",
      "‚è≥ Waiting 7.0 seconds before next request...\n",
      "\n",
      "üéâ Finished updating all records.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import requests\n",
    "import time, random\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def enrich_references_with_crossref(db_path=\"references.db\", table=\"references\", topn=3):\n",
    "    \"\"\"\n",
    "    Sequentially enrich all rows in the SQLite DB with Crossref BibTeX and Title similarity.\n",
    "    Always keeps the best candidate among top N results.\n",
    "    Title_Similarity is stored as percentage (0‚Äì100).\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Ensure columns exist\n",
    "    cur.execute(f'PRAGMA table_info(\"{table}\")')\n",
    "    existing_cols = [col[1] for col in cur.fetchall()]\n",
    "    if \"Crossref_BibTeX\" not in existing_cols:\n",
    "        cur.execute(f'ALTER TABLE \"{table}\" ADD COLUMN Crossref_BibTeX TEXT;')\n",
    "    if \"Title_Similarity\" not in existing_cols:\n",
    "        cur.execute(f'ALTER TABLE \"{table}\" ADD COLUMN Title_Similarity REAL;')\n",
    "    conn.commit()\n",
    "\n",
    "    # Fetch all rows\n",
    "    cur.execute(f'SELECT Reference, Title, Authors, \"Journal/Booktitle\", Year, Publisher FROM \"{table}\"')\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    for i, (ref, title, authors, journal, year, publisher) in enumerate(rows, start=1):\n",
    "        print(f\"\\n[{i}/{len(rows)}] Processing Reference={ref}\")\n",
    "\n",
    "        if not title:\n",
    "            print(\"‚ö†Ô∏è No title found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Build query string\n",
    "        query = title\n",
    "        if authors: query += f\" {authors.split(',')[0]}\"\n",
    "        if journal: query += f\" {journal}\"\n",
    "        if year: query += f\" {year}\"\n",
    "        if publisher: query += f\" {publisher}\"\n",
    "\n",
    "        url = f\"https://api.crossref.org/works?query.bibliographic={requests.utils.quote(query)}&rows={topn}\"\n",
    "\n",
    "        crossref_bibtex, best_score = \"\", 0\n",
    "        try:\n",
    "            r = requests.get(url, timeout=15)\n",
    "            r.raise_for_status()\n",
    "            items = r.json().get(\"message\", {}).get(\"items\", [])\n",
    "\n",
    "            if not items:\n",
    "                print(\"‚ö†Ô∏è No Crossref results found.\")\n",
    "            else:\n",
    "                best = None\n",
    "                for item in items:\n",
    "                    cr_title = item.get(\"title\", [\"\"])[0]\n",
    "                    score = SequenceMatcher(None, title.lower(), cr_title.lower()).ratio()\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best = item\n",
    "\n",
    "                # Convert to percentage (0‚Äì100)\n",
    "                best_score = round(best_score * 100, 2)\n",
    "\n",
    "                if best:\n",
    "                    doi = best.get(\"DOI\", \"\")\n",
    "                    if doi:\n",
    "                        bibtex_r = requests.get(\n",
    "                            f\"https://doi.org/{doi}\",\n",
    "                            headers={\"Accept\": \"application/x-bibtex\"},\n",
    "                            timeout=15\n",
    "                        )\n",
    "                        if bibtex_r.status_code == 200:\n",
    "                            crossref_bibtex = bibtex_r.text.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Crossref fetch failed for {ref}: {e}\")\n",
    "\n",
    "        # Update DB immediately\n",
    "        cur.execute(\n",
    "            f'UPDATE \"{table}\" SET Crossref_BibTeX=?, Title_Similarity=? WHERE Reference=?',\n",
    "            (crossref_bibtex, best_score, ref)\n",
    "        )\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"‚úÖ Updated: Similarity={best_score}%, BibTeX length={len(crossref_bibtex)}\")\n",
    "\n",
    "        # Random delay\n",
    "        sleep_time = random.uniform(2, 5)\n",
    "        print(f\"‚è≥ Waiting {sleep_time:.1f} seconds before next request...\")\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    conn.close()\n",
    "    print(\"\\nüéâ Finished updating all records.\")\n",
    "\n",
    "\n",
    "enrich_references_with_crossref(\"references.db\", \"references\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55966c41",
   "metadata": {},
   "source": [
    "# Abbreviation (Elsevier Publisher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a7bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Column Journal_Abbrev already exists, skipping schema change.\n",
      "‚úÖ Journal abbreviations added next to Journal/Booktitle\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def load_ltwa_mapping(csv_path: str) -> dict:\n",
    "    df = pd.read_csv(csv_path, sep='\\t', engine='python', dtype=str)\n",
    "    df = df.dropna(subset=['ABBREVIATION'])\n",
    "    df_en = df[df['LANGUAGES'].str.contains('English', case=False, na=False)]\n",
    "    return {row['WORD'].lower(): row['ABBREVIATION'] for _, row in df_en.iterrows()}\n",
    "\n",
    "def find_abbreviation(word: str, mapping: dict) -> str:\n",
    "    key = word.lower()\n",
    "    if key in mapping and mapping[key]:\n",
    "        return mapping[key]\n",
    "    dash_key = f\"{key}-\"\n",
    "    if dash_key in mapping and mapping[dash_key]:\n",
    "        return mapping[dash_key]\n",
    "    truncated = key\n",
    "    while len(truncated) > 1:\n",
    "        truncated = truncated[:-1]\n",
    "        tr_key = f\"{truncated}-\"\n",
    "        if tr_key in mapping and mapping[tr_key]:\n",
    "            return mapping[tr_key]\n",
    "    return word\n",
    "\n",
    "def abbreviate_journal_title(title: str, mapping: dict) -> str:\n",
    "    tokens = re.split(r'(\\W+)', title or \"\")\n",
    "    return ''.join(\n",
    "        find_abbreviation(tok, mapping) if re.match(r'\\w+', tok) else tok\n",
    "        for tok in tokens\n",
    "    )\n",
    "\n",
    "def add_journal_abbreviations(db_path=\"references.db\", table=\"references\", ltwa_csv=\"ltwa.tsv\"):\n",
    "    \"\"\"\n",
    "    Add a new column 'Journal_Abbrev' immediately after 'Journal/Booktitle'\n",
    "    in the SQLite DB, filled with English LTWA abbreviations.\n",
    "    \"\"\"\n",
    "    # Load abbreviation mapping (English only)\n",
    "    mapping = load_ltwa_mapping(ltwa_csv)\n",
    "\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Get schema of the current table\n",
    "    cur.execute(f'PRAGMA table_info(\"{table}\")')\n",
    "    cols_info = cur.fetchall()\n",
    "    col_names = [c[1] for c in cols_info]\n",
    "\n",
    "    if \"Journal_Abbrev\" in col_names:\n",
    "        print(\"‚ÑπÔ∏è Column Journal_Abbrev already exists, skipping schema change.\")\n",
    "    else:\n",
    "        # Rebuild table with new column in correct place\n",
    "        new_cols = []\n",
    "        for name in col_names:\n",
    "            new_cols.append(name)\n",
    "            if name == \"Journal/Booktitle\":\n",
    "                new_cols.append(\"Journal_Abbrev\")\n",
    "        col_defs = \", \".join(\n",
    "            f'\"{c}\" TEXT' if c not in (\"Frequency\", \"Year\") else f'\"{c}\" INTEGER'\n",
    "            for c in new_cols\n",
    "        )\n",
    "\n",
    "        # Create temp table\n",
    "        cur.execute(f'CREATE TABLE \"{table}_new\" ({col_defs});')\n",
    "\n",
    "        # Copy data into new table with empty Journal_Abbrev\n",
    "        select_expr = \", \".join(\n",
    "            [f'\"{c}\"' if c != \"Journal_Abbrev\" else \"NULL\" for c in new_cols]\n",
    "        )\n",
    "        cur.execute(f'INSERT INTO \"{table}_new\" SELECT {select_expr} FROM \"{table}\";')\n",
    "\n",
    "        # Replace old table\n",
    "        cur.execute(f'DROP TABLE \"{table}\";')\n",
    "        cur.execute(f'ALTER TABLE \"{table}_new\" RENAME TO \"{table}\";')\n",
    "        conn.commit()\n",
    "\n",
    "    # Now fill the abbreviations\n",
    "    cur.execute(f'SELECT Reference, \"Journal/Booktitle\" FROM \"{table}\"')\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    for ref, journal in rows:\n",
    "        abbrev = abbreviate_journal_title(journal, mapping) if journal else \"\"\n",
    "        cur.execute(\n",
    "            f'UPDATE \"{table}\" SET Journal_Abbrev=? WHERE Reference=?',\n",
    "            (abbrev, ref)\n",
    "        )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"‚úÖ Journal abbreviations added next to Journal/Booktitle\")\n",
    "\n",
    "add_journal_abbreviations(\n",
    "    db_path=\"references.db\",\n",
    "    table=\"references\",\n",
    "    ltwa_csv=\"ltwa.txt\"   # path to your LTWA abbreviation list\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba3e4fe",
   "metadata": {},
   "source": [
    "# Final step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61750265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crossref_BibTeX_Abbrev fixed with correct keys and journals\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import re\n",
    "\n",
    "def add_crossref_bibtex_with_abbrev(db_path=\"references.db\", table=\"references\"):\n",
    "    \"\"\"\n",
    "    Create a new column 'Crossref_BibTeX_Abbrev' where:\n",
    "      - BibTeX key is replaced by the 'Reference' column value\n",
    "      - Journal name is replaced by the 'Journal_Abbrev' column value\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Ensure column exists\n",
    "    cur.execute(f'PRAGMA table_info(\"{table}\")')\n",
    "    existing_cols = [col[1] for col in cur.fetchall()]\n",
    "    if \"Crossref_BibTeX_Abbrev\" not in existing_cols:\n",
    "        cur.execute(f'ALTER TABLE \"{table}\" ADD COLUMN Crossref_BibTeX_Abbrev TEXT;')\n",
    "        conn.commit()\n",
    "\n",
    "    cur.execute(f'SELECT Reference, Journal_Abbrev, Crossref_BibTeX FROM \"{table}\"')\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    for ref, journal_abbrev, crossref_bib in rows:\n",
    "        if not crossref_bib:\n",
    "            continue\n",
    "\n",
    "        new_bib = crossref_bib.strip()\n",
    "\n",
    "        # --- 1) Replace BibTeX key safely ---\n",
    "        try:\n",
    "            start_brace = new_bib.index(\"{\")\n",
    "            first_comma = new_bib.index(\",\", start_brace)\n",
    "            entry_type = new_bib[:start_brace]        # e.g. \"@article\"\n",
    "            # rebuild entry start\n",
    "            new_start = f\"{entry_type}{{{ref},\"\n",
    "            # replace old start with new start\n",
    "            new_bib = new_start + new_bib[first_comma+1:]\n",
    "        except ValueError:\n",
    "            # fallback if format is unexpected\n",
    "            pass\n",
    "\n",
    "        # --- 2) Replace journal field with abbreviation ---\n",
    "        if journal_abbrev:\n",
    "            new_bib = re.sub(\n",
    "                r'(journal\\s*=\\s*\\{)[^}]+(\\})',\n",
    "                rf'\\1{journal_abbrev}\\2',\n",
    "                new_bib,\n",
    "                flags=re.IGNORECASE\n",
    "            )\n",
    "\n",
    "        # Update DB\n",
    "        cur.execute(\n",
    "            f'UPDATE \"{table}\" SET Crossref_BibTeX_Abbrev=? WHERE Reference=?',\n",
    "            (new_bib, ref)\n",
    "        )\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"‚úÖ Crossref_BibTeX_Abbrev fixed with correct keys and journals\")\n",
    "\n",
    "add_crossref_bibtex_with_abbrev(\"references.db\", \"references\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
