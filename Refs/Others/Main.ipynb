{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12594f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./Final Step/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b864161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸ Cleaning Crossref_BibTeX_Protected in Refs.db:Refs\n",
      "âœ… Updated chen2020renewable\n",
      "âœ… Updated somu2021deep\n",
      "âœ… Updated hassan2016systematic\n",
      "âœ… Updated lin2022hybrid\n",
      "âœ… Updated 10734172\n",
      "âœ… Updated al2023hybrid\n",
      "âœ… Updated kim2023novel\n",
      "âœ… Updated fekri2021deep\n",
      "âœ… Updated noorchenarboo2025explaining\n",
      "âœ… Updated antwarg2021explaining\n",
      "âœ… Updated chen2023algorithms\n",
      "âœ… Updated dehrouyeh2024tinyml\n",
      "âœ… Updated aras2022interpretable\n",
      "âœ… Updated lundberg2017unified\n",
      "âœ… Updated dehrouyeh2025pruning\n",
      "âœ… Updated van2024harnessing\n",
      "âœ… Updated jethani2021fastshap\n",
      "âœ… Updated gao2021interpretable\n",
      "âœ… Updated ozcan2021energy\n",
      "âœ… Updated gonccalves2023variable\n",
      "âœ… Updated han2024short\n",
      "âœ… Updated peng2024explainable\n",
      "âœ… Updated choi2022explainable\n",
      "âœ… Updated lu2024incorporating\n",
      "âœ… Updated wang2024shap\n",
      "âœ… Updated zhou2024deciphering\n",
      "âœ… Updated amiri2023investigating\n",
      "âœ… Updated aas2021explaining\n",
      "âœ… Updated weather_canada_historical_data\n",
      "âœ… Updated Miller2020yc\n",
      "ðŸŽ‰ Done cleaning Crossref_BibTeX_Protected\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import re\n",
    "\n",
    "def protect_acronyms_in_fields(bibtex: str) -> str:\n",
    "    def clean_field(field_name: str, text: str) -> str:\n",
    "        text = text.strip()\n",
    "        def wrap_token(token: str) -> str:\n",
    "            if token.startswith(\"{\") and token.endswith(\"}\"):\n",
    "                return token\n",
    "            if sum(1 for c in token if c.isupper()) >= 2:\n",
    "                return \"{\" + token + \"}\"\n",
    "            return token\n",
    "        tokens = re.split(r'(\\s+)', text)\n",
    "        fixed = \"\".join(wrap_token(tok) if tok.strip() else tok for tok in tokens)\n",
    "        fixed = re.sub(r'\\{\\{([^{}]+)\\}\\}', r'{\\1}', fixed)\n",
    "        return f\"{field_name}={{{fixed.strip()}}}\"\n",
    "\n",
    "    for field in [\"title\", \"booktitle\", \"journal\"]:\n",
    "        bibtex = re.sub(\n",
    "            rf'{field}\\s*=\\s*\\{{([^}}]*)\\}}',\n",
    "            lambda m: clean_field(field, m.group(1)),\n",
    "            bibtex,\n",
    "            flags=re.IGNORECASE\n",
    "        )\n",
    "    return bibtex\n",
    "\n",
    "def normalize_braces_spacing(bibtex: str) -> str:\n",
    "    if not bibtex:\n",
    "        return bibtex\n",
    "    bibtex = bibtex.replace(\"}} \", \"} \")\n",
    "    bibtex = bibtex.replace(\"{{ \", \"{ \")\n",
    "    return bibtex\n",
    "\n",
    "def clean_crossref_bibtex_protected(db_path=\"Refs.db\", table=\"Refs\"):\n",
    "    print(f\"ðŸ› ï¸ Cleaning Crossref_BibTeX_Protected in {db_path}:{table}\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f'PRAGMA table_info(\"{table}\")')\n",
    "    cols = [c[1] for c in cur.fetchall()]\n",
    "    if \"Crossref_BibTeX_Protected\" not in cols:\n",
    "        cur.execute(f'ALTER TABLE \"{table}\" ADD COLUMN Crossref_BibTeX_Protected TEXT;')\n",
    "        conn.commit()\n",
    "\n",
    "    cur.execute(f'SELECT Reference, Crossref_BibTeX_Abbrev FROM \"{table}\"')\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    for ref, bib in rows:\n",
    "        if not bib:\n",
    "            continue\n",
    "        cleaned = protect_acronyms_in_fields(bib)\n",
    "        cleaned = normalize_braces_spacing(cleaned)\n",
    "        cur.execute(\n",
    "            f'UPDATE \"{table}\" SET Crossref_BibTeX_Protected=? WHERE Reference=?',\n",
    "            (cleaned, ref)\n",
    "        )\n",
    "        print(f\"âœ… Updated {ref}\")\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"ðŸŽ‰ Done cleaning Crossref_BibTeX_Protected\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_crossref_bibtex_protected(\"Refs.db\", \"Refs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0a7f70",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "intoduction.txt -> intoduction.bib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dce5a680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 4 BibTeX entries to input.bib\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_bibtex_entries():\n",
    "    \"\"\"\n",
    "    Extract all BibTeX entries from raw.txt and save them to input.bib\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the raw.txt file\n",
    "        with open('intoduction.txt', 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        # Improved regex pattern to match multi-line BibTeX entries\n",
    "        # This pattern matches from @ to the closing brace, handling multi-line entries\n",
    "        bibtex_pattern = r'@\\w+\\{[\\s\\S]*?\\n\\}'\n",
    "        \n",
    "        # Find all BibTeX entries\n",
    "        bibtex_entries = re.findall(bibtex_pattern, content)\n",
    "        \n",
    "        if not bibtex_entries:\n",
    "            print(\"No BibTeX entries found in the file.\")\n",
    "            return None\n",
    "        \n",
    "        # Write BibTeX entries to input.bib\n",
    "        with open('intoduction.bib', 'w', encoding='utf-8') as output_file:\n",
    "            for i, entry in enumerate(bibtex_entries):\n",
    "                # Write the entry as-is (it's already properly formatted)\n",
    "                output_file.write(entry.strip())\n",
    "                \n",
    "                # Add a blank line between entries (except for the last one)\n",
    "                if i < len(bibtex_entries) - 1:\n",
    "                    output_file.write('\\n\\n')\n",
    "        \n",
    "        print(f\"Successfully extracted {len(bibtex_entries)} BibTeX entries to input.bib\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: File 'raw.txt' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {str(e)}\")\n",
    "\n",
    "extract_bibtex_entries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bdcd6d",
   "metadata": {},
   "source": [
    "# Literature review\n",
    "Scopus.bib -> Scopus_Clean.bib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b260c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned BibTeX file saved to: Scopus_Cleaned.bib\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "ENTRY_SPLIT_RE = re.compile(r'@(?=[a-zA-Z]+\\s*\\{)', re.MULTILINE)\n",
    "\n",
    "def clean_bibtex_file(input_path, output_path):\n",
    "    with open(input_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    cleaned_lines = []\n",
    "    current_entry = []\n",
    "    is_wanted_entry = False\n",
    "    skip_keywords = ['url =', 'source =']\n",
    "\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "\n",
    "        if stripped.startswith('@'):\n",
    "            if current_entry and is_wanted_entry:\n",
    "                cleaned_lines.extend(current_entry)\n",
    "            current_entry = []\n",
    "            entry_type = stripped[1:].split('{,', 1)[0].split('{',1)[0].lower()\n",
    "            is_wanted_entry = entry_type in ('article', 'conference')\n",
    "            if is_wanted_entry:\n",
    "                m = re.match(r'^(@\\w+\\{)([^,]+)(,.*)$', stripped)\n",
    "                if m:\n",
    "                    prefix, raw_key, suffix = m.groups()\n",
    "                    clean_key = re.sub(r'[^0-9A-Za-z]', '', raw_key)\n",
    "                    line = f\"{prefix}{clean_key}{suffix}\\n\"\n",
    "\n",
    "        if is_wanted_entry and not any(kw in stripped for kw in skip_keywords):\n",
    "            current_entry.append(line)\n",
    "\n",
    "    if current_entry and is_wanted_entry:\n",
    "        cleaned_lines.extend(current_entry)\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(cleaned_lines)\n",
    "\n",
    "    print(f\"Cleaned BibTeX file saved to: {output_path}\")\n",
    "\n",
    "def _validate_table_name(name: str) -> str:\n",
    "    if not re.match(r'^[A-Za-z_][A-Za-z0-9_]*$', name or ''):\n",
    "        raise ValueError(\"Invalid table name. Use letters, numbers, and underscores; must not start with a number.\")\n",
    "    return name\n",
    "\n",
    "def _ensure_table(db_path: str, table_name: str):\n",
    "    table_name = _validate_table_name(table_name)\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS \"{table_name}\" (\n",
    "            type TEXT,\n",
    "            bibtex_key TEXT UNIQUE,\n",
    "            author TEXT,\n",
    "            title TEXT,\n",
    "            year TEXT,\n",
    "            journal TEXT,\n",
    "            doi TEXT,\n",
    "            publisher TEXT,\n",
    "            citation_number INTEGER,\n",
    "            abstract TEXT,\n",
    "            raw_bibtex TEXT,\n",
    "            summary TEXT,\n",
    "            keyword TEXT,\n",
    "            used TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def _split_entries(cleaned_text: str) -> list[str]:\n",
    "    chunks = ENTRY_SPLIT_RE.split(cleaned_text)\n",
    "    entries = []\n",
    "    for c in chunks:\n",
    "        c = c.strip()\n",
    "        if not c:\n",
    "            continue\n",
    "        if not c.startswith('@'):\n",
    "            c = '@' + c\n",
    "        entries.append(c)\n",
    "    return entries\n",
    "\n",
    "def _extract_field(entry: str, field: str):\n",
    "    pattern = rf'(?im)^\\s*{re.escape(field)}\\s*=\\s*[{{\"]([^}}\"]+)[}}\"]'\n",
    "    m = re.search(pattern, entry)\n",
    "    return m.group(1).strip() if m else None\n",
    "\n",
    "def _extract_type_and_key(entry: str):\n",
    "    m = re.match(r'@(\\w+)\\s*{\\s*([^,]+)', entry, re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1), m.group(2)\n",
    "    return None, None\n",
    "\n",
    "def _extract_citation_number(entry: str):\n",
    "    note_val = _extract_field(entry, \"note\")\n",
    "    if note_val:\n",
    "        m = re.search(r\"Cited by:\\s*(\\d+)\", note_val, re.IGNORECASE)\n",
    "        if m:\n",
    "            return int(m.group(1))\n",
    "    return None\n",
    "\n",
    "def save_run(db_path: str,\n",
    "             table_name: str,\n",
    "             keyword: str,\n",
    "             input_bib: str = \"Scopus.bib\",\n",
    "             output_bib: str = \"Scopus_cleaned.bib\"):\n",
    "    \"\"\"\n",
    "    Save each BibTeX entry as a separate row into a user-defined table.\n",
    "    Deduplicates by bibtex_key (only inserts new keys).\n",
    "    \"\"\"\n",
    "    clean_bibtex_file(input_bib, output_bib)\n",
    "\n",
    "    cleaned_text = Path(output_bib).read_text(encoding=\"utf-8\")\n",
    "    _ensure_table(db_path, table_name)\n",
    "    entries = _split_entries(cleaned_text)\n",
    "\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    for entry in entries:\n",
    "        paper_type, bibtex_key = _extract_type_and_key(entry)\n",
    "        if not bibtex_key:\n",
    "            continue\n",
    "\n",
    "        cur.execute(f'SELECT 1 FROM \"{table_name}\" WHERE bibtex_key = ?', (bibtex_key,))\n",
    "        if cur.fetchone():\n",
    "            continue  # skip existing\n",
    "\n",
    "        cur.execute(f\"\"\"\n",
    "            INSERT INTO \"{table_name}\" (\n",
    "                type, bibtex_key, author, title, year, journal, doi, publisher,\n",
    "                citation_number, abstract, raw_bibtex, summary, keyword, used\n",
    "            )\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", (\n",
    "            paper_type,\n",
    "            bibtex_key,\n",
    "            _extract_field(entry, \"author\"),\n",
    "            _extract_field(entry, \"title\"),\n",
    "            _extract_field(entry, \"year\"),\n",
    "            _extract_field(entry, \"journal\"),\n",
    "            _extract_field(entry, \"doi\"),\n",
    "            _extract_field(entry, \"publisher\"),\n",
    "            _extract_citation_number(entry),\n",
    "            _extract_field(entry, \"abstract\"),\n",
    "            entry,\n",
    "            None,       # summary empty\n",
    "            keyword,    # keyword at the end\n",
    "            None        # used empty\n",
    "        ))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def clear_sqlite_sequence(db_path: str, table: str | None = None):\n",
    "    \"\"\"\n",
    "    Clears rows from the sqlite_sequence system table.\n",
    "    NOTE: You can't drop sqlite_sequence; you can only delete its rows.\n",
    "    It only matters if you previously used AUTOINCREMENT. This schema doesn't.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    # sqlite_sequence may not exist if never used; guard it\n",
    "    cur.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='sqlite_sequence'\")\n",
    "    exists = cur.fetchone()\n",
    "    if not exists:\n",
    "        conn.close()\n",
    "        return\n",
    "\n",
    "    if table:\n",
    "        cur.execute(\"DELETE FROM sqlite_sequence WHERE name = ?\", (table,))\n",
    "    else:\n",
    "        cur.execute(\"DELETE FROM sqlite_sequence\")\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Choose your own table name:\n",
    "save_run(\n",
    "    db_path=\"CaseTime_project1_intro.db\",\n",
    "    table_name=\"Introduction\",   # <-- Related_Works or Introduction\n",
    "    keyword=\"p4\",\n",
    "    input_bib=\"Scopus.bib\",\n",
    "    output_bib=\"Scopus_Cleaned.bib\"\n",
    ")\n",
    "\n",
    "# # (Optional) Clear AUTOINCREMENT remnants if sqlite_sequence exists:\n",
    "# clear_sqlite_sequence(\"paper3.db\")          # clear all\n",
    "# # or\n",
    "# clear_sqlite_sequence(\"paper3.db\", \"papers\")  # clear for a specific old table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afe2ec9",
   "metadata": {},
   "source": [
    "# Order cite by section from latex file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efad9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Headings found: 17 | Blocks: 18\n",
      "[DEBUG] Total citation occurrences (per block): 34\n",
      "[DEBUG] Unique cited keys: 34\n",
      "[DEBUG] Parsed BibTeX entries: 76 from Refs.bib\n",
      "[DEBUG] First few keys: fekri2021deep, noorchenarboo2025explaining, daut2017building, ahmad2014review, ramos2023residential, ghalehkhondabi2017overview, zeng2017multifactor, zheng2023interpretable, jin2022highly, chou2018forecasting\n",
      "Done! Wrote 0 entries to cited_by_section.bib\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def split_bib_by_section(tex_path: str, bib_path: str, out_bib_path: str):\n",
    "    r\"\"\"\n",
    "    Reads a LaTeX file (tex_path) and a BibTeX file (bib_path),\n",
    "    then writes a new BibTeX file (out_bib_path) containing only\n",
    "    those entries first cited in each \\section{â€¦} of the .tex,\n",
    "    grouped and commented by section, **with no empty lines inside entries**.\n",
    "    \"\"\"\n",
    "    # 1. Read files\n",
    "    tex = open(tex_path,  encoding='utf-8').read()\n",
    "    bib = open(bib_path, encoding='utf-8').read()\n",
    "\n",
    "    # 2. Split into sections\n",
    "    sec_re = re.compile(r'\\\\section\\{([^}]+)\\}')\n",
    "    matches = list(sec_re.finditer(tex))\n",
    "    sections = []\n",
    "    if matches and matches[0].start() > 0:\n",
    "        sections.append(('Preamble', tex[:matches[0].start()]))\n",
    "    for i, m in enumerate(matches):\n",
    "        title = m.group(1).strip()\n",
    "        start = m.end()\n",
    "        end   = matches[i+1].start() if i+1 < len(matches) else len(tex)\n",
    "        sections.append((title, tex[start:end]))\n",
    "    if not matches:\n",
    "        sections = [('Document', tex)]\n",
    "\n",
    "    # 3. Gather first-time citations per section\n",
    "    cited = set()\n",
    "    section_order = []\n",
    "    cite_re = re.compile(r'\\\\cite\\w*\\{([^}]+)\\}')\n",
    "    for title, content in sections:\n",
    "        keys_in_sec = []\n",
    "        for g in cite_re.findall(content):\n",
    "            for key in g.split(','):\n",
    "                key = key.strip()\n",
    "                if key and key not in cited:\n",
    "                    cited.add(key)\n",
    "                    keys_in_sec.append(key)\n",
    "        if keys_in_sec:\n",
    "            section_order.append((title, keys_in_sec))\n",
    "\n",
    "    # 4. Load all Bib entries into a dict\n",
    "    bib_entries = {}\n",
    "    for entry in re.split(r'(?=@)', bib):\n",
    "        m = re.match(r'@\\w+\\{([^,]+),', entry)\n",
    "        if m:\n",
    "            bib_entries[m.group(1).strip()] = entry.strip()\n",
    "\n",
    "    # 5. Write filtered .bib (cleaning out blank lines within entries)\n",
    "    with open(out_bib_path, 'w', encoding='utf-8') as out:\n",
    "        for title, keys in section_order:\n",
    "            out.write(f\"% Section: {title}\\n\")\n",
    "            for k in keys:\n",
    "                raw_entry = bib_entries.get(k)\n",
    "                if raw_entry:\n",
    "                    # remove any entirely blank lines\n",
    "                    lines = raw_entry.splitlines()\n",
    "                    clean = [ln for ln in lines if ln.strip()]\n",
    "                    out.write(\"\\n\".join(clean) + \"\\n\\n\")\n",
    "                else:\n",
    "                    out.write(f\"% WARNING: no entry for '{k}'\\n\\n\")\n",
    "\n",
    "    print(f\"Done! {len(cited)} unique citations written to {out_bib_path}\")\n",
    "\n",
    "split_bib_by_section(\n",
    "    tex_path='main.tex',\n",
    "    bib_path='Refs.bib',\n",
    "    out_bib_path='cited_by_section.bib'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29efa00b",
   "metadata": {},
   "source": [
    "# Update by CrossREF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4a9315c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ—] No exact match for â€œBig data and healthâ€, keeping original.\n",
      "[âœ—] No exact match for â€œPublicly available clinical BERT embeddingsâ€, keeping original.\n",
      "[âœ—] No exact match for â€œEffect of a Predictive Model on Planned Surgical Duration Accuracy, Patient Wait Time, and Use of Presurgical Resources: A Randomized Clinical Trialâ€, keeping original.\n",
      "\n",
      "âœ… Updated .bib written to: crossRef_cited_by_section.bib\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import string\n",
    "\n",
    "def normalize_title(t: str) -> str:\n",
    "    \"\"\"\n",
    "    Lowercase, strip out everything except letters, digits and spaces,\n",
    "    collapse multiple spaces into one.\n",
    "    \"\"\"\n",
    "    allowed = set(string.ascii_lowercase + string.digits + \" \")\n",
    "    t = t.lower()\n",
    "    t = \"\".join(ch for ch in t if ch in allowed)\n",
    "    return \" \".join(t.split())\n",
    "\n",
    "def update_bib_with_crossref_strict(input_bib_path: str, output_bib_path: str):\n",
    "    text = open(input_bib_path, encoding='utf-8').read()\n",
    "    # split by your Section headers\n",
    "    blocks = re.split(r'(?=%\\s*Section:)', text)\n",
    "    cache = {}\n",
    "    out = []\n",
    "\n",
    "    for block in blocks:\n",
    "        if not block.strip():\n",
    "            continue\n",
    "        # keep anything before the first \"% Section:\"\n",
    "        if not block.lstrip().startswith('% Section:'):\n",
    "            out.append(block)\n",
    "            continue\n",
    "\n",
    "        # Separate header line from the entries\n",
    "        header, *rest = block.splitlines()\n",
    "        header += \"\\n\"\n",
    "        content = \"\\n\".join(rest)\n",
    "        raw_entries = re.split(r'(?=@\\w+\\{)', content)\n",
    "        updated_entries = []\n",
    "\n",
    "        for raw in raw_entries:\n",
    "            entry = raw.strip()\n",
    "            if not entry or not entry.startswith('@'):\n",
    "                continue\n",
    "\n",
    "            # pull out entry type and your original key\n",
    "            m = re.match(r'@(\\w+)\\{([^,]+),', entry)\n",
    "            if not m:\n",
    "                updated_entries.append(entry)\n",
    "                continue\n",
    "            ent_type, orig_key = m.group(1), m.group(2)\n",
    "\n",
    "            # pull title / authors / year\n",
    "            t_m = re.search(r'title\\s*=\\s*\\{([^}]+)\\}', entry, re.I)\n",
    "            a_m = re.search(r'author\\s*=\\s*\\{([^}]+)\\}', entry, re.I)\n",
    "            y_m = re.search(r'year\\s*=\\s*\\{?(\\d{4})\\}?', entry, re.I)\n",
    "\n",
    "            if not t_m:\n",
    "                print(f\"[âœ—] No title for key '{orig_key}', keeping original.\")\n",
    "                updated_entries.append(entry)\n",
    "                continue\n",
    "\n",
    "            raw_title = t_m.group(1).strip()\n",
    "            norm_title = normalize_title(raw_title)\n",
    "\n",
    "            # if we've done this title before, reuse whatever we got\n",
    "            if norm_title in cache:\n",
    "                updated_entries.append(cache[norm_title])\n",
    "                continue\n",
    "\n",
    "            doi = None\n",
    "            # 1) titleâ€based search\n",
    "            try:\n",
    "                r1 = requests.get(\n",
    "                    'https://api.crossref.org/works',\n",
    "                    params={'query.title': raw_title, 'rows': 1}\n",
    "                )\n",
    "                if r1.ok:\n",
    "                    items = r1.json().get('message', {}).get('items', [])\n",
    "                    if items:\n",
    "                        cand = items[0]\n",
    "                        cand_title = cand.get('title', [''])[0]\n",
    "                        if normalize_title(cand_title) == norm_title:\n",
    "                            doi = cand.get('DOI')\n",
    "            except Exception as e:\n",
    "                print(f\"[âœ—] Titleâ€search error for '{raw_title}': {e}\")\n",
    "\n",
    "            # 2) fallback author+year, but **still** require title match\n",
    "            if not doi and a_m and y_m:\n",
    "                first_author = a_m.group(1).split(' and ')[0].split()[-1]\n",
    "                year = y_m.group(1)\n",
    "                try:\n",
    "                    r2 = requests.get(\n",
    "                        'https://api.crossref.org/works',\n",
    "                        params={\n",
    "                            'query.author': first_author,\n",
    "                            'filter': f'from-pub-date:{year},until-pub-date:{year}',\n",
    "                            'rows': 1\n",
    "                        }\n",
    "                    )\n",
    "                    if r2.ok:\n",
    "                        items2 = r2.json().get('message', {}).get('items', [])\n",
    "                        if items2:\n",
    "                            cand = items2[0]\n",
    "                            cand_title2 = cand.get('title', [''])[0]\n",
    "                            if normalize_title(cand_title2) == norm_title:\n",
    "                                doi = cand.get('DOI')\n",
    "                except Exception as e:\n",
    "                    print(f\"[âœ—] Fallback error for '{raw_title}': {e}\")\n",
    "\n",
    "            # 3) if we got a DOI, fetch fresh BibTeX and force your key\n",
    "            if doi:\n",
    "                try:\n",
    "                    r3 = requests.get(\n",
    "                        f'https://doi.org/{doi}',\n",
    "                        headers={'Accept': 'application/x-bibtex; charset=utf-8'}\n",
    "                    )\n",
    "                    if r3.ok:\n",
    "                        fresh = r3.text\n",
    "                        # swap in your original key\n",
    "                        fresh = re.sub(\n",
    "                            r'(@\\w+\\{)[^,]+,',\n",
    "                            fr'\\1{orig_key},',\n",
    "                            fresh, count=1\n",
    "                        )\n",
    "                        new_entry = fresh.strip()\n",
    "                    else:\n",
    "                        print(f\"[âœ—] Couldnâ€™t fetch BibTeX for DOI {doi}, keeping original.\")\n",
    "                        new_entry = entry\n",
    "                except Exception as e:\n",
    "                    print(f\"[âœ—] Fetch error for DOI {doi}: {e}\")\n",
    "                    new_entry = entry\n",
    "            else:\n",
    "                # no valid DOI/titleâ€match\n",
    "                print(f\"[âœ—] No exact match for â€œ{raw_title}â€, keeping original.\")\n",
    "                new_entry = entry\n",
    "\n",
    "            cache[norm_title] = new_entry\n",
    "            updated_entries.append(new_entry)\n",
    "            time.sleep(random.uniform(10, 30))  # be polite\n",
    "\n",
    "        # reassemble this section\n",
    "        out.append(header)\n",
    "        for e in updated_entries:\n",
    "            out.append(e + \"\\n\\n\")\n",
    "\n",
    "    # write out\n",
    "    with open(output_bib_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\".join(out))\n",
    "\n",
    "    print(f\"\\nâœ… Updated .bib written to: {output_bib_path}\")\n",
    "\n",
    "\n",
    "update_bib_with_crossref_strict(\n",
    "    input_bib_path='cited_by_section.bib',\n",
    "    output_bib_path='crossRef_cited_by_section.bib'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea3382b",
   "metadata": {},
   "source": [
    "# Abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f156c2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced journal title:\n",
      "  Old: JAMA Surgery\n",
      "  New: JAMA surg.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Mayo Clinic Proceedings: Innovations, Quality &amp; Outcomes\n",
      "  New: Mayo clin. proc.: Innovations, qual. &amp; Outcomes\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: European Journal of Operational Research\n",
      "  New: European j. of Operational res.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Computer Methods and Programs in Biomedicine\n",
      "  New: comput. Methods and Programs in biomed.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Journal of Combinatorial Optimization\n",
      "  New: j. of comb. optim.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: The Lancet Digital Health\n",
      "  New: The Lancet Digital heal.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Journal of Medical Systems\n",
      "  New: j. of med. syst.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Surgical Innovation\n",
      "  New: surg. Innovation\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Nature Communications\n",
      "  New: nat. commun.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: BMC Medical Research Methodology\n",
      "  New: BMC med. res. methodol.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: arXiv preprint arXiv:1904.03323\n",
      "  New: arXiv prepr. arXiv:1904.03323\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Journal of the American College of Surgeons\n",
      "  New: j. of the am. coll. of surg.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Computer Methods and Programs in Biomedicine\n",
      "  New: comput. Methods and Programs in biomed.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Journal of Medical Systems\n",
      "  New: j. of med. syst.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Otolaryngologyâ€“Head and Neck Surgery\n",
      "  New: Otolaryngologyâ€“Head and Neck surg.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Archives of Orthopaedic and Trauma Surgery\n",
      "  New: Archives of orthop. and Trauma surg.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Annals of Surgery\n",
      "  New: ann. of surg.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Journal of Medical Systems\n",
      "  New: j. of med. syst.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Surgical Endoscopy\n",
      "  New: surg. endosc.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: JAMA Surgery\n",
      "  New: JAMA surg.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Journal of Biomedical Informatics\n",
      "  New: j. of biomed. Informatics\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: BMC Medical Informatics and Decision Making\n",
      "  New: BMC med. Informatics and decis. mak.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Artificial Intelligence in Medicine\n",
      "  New: artif. intell. in Medicine\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Journal of the American Medical Informatics Association\n",
      "  New: j. of the am. med. Informatics assoc.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Journal of Healthcare Informatics Research\n",
      "  New: j. of healthc. Informatics res.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: JMIR Medical Informatics\n",
      "  New: JMIR med. Informatics\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Journal of the American Medical Informatics Association\n",
      "  New: j. of the am. med. Informatics assoc.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Computer Methods and Programs in Biomedicine\n",
      "  New: comput. Methods and Programs in biomed.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Surgery\n",
      "  New: surg.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Arthroplasty Today\n",
      "  New: arthroplast. Today\n",
      "\n",
      "âœ… Abbreviated journals written to 'abbrev_crossRef_cited_by_section.bib'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def load_ltwa_mapping(csv_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load LTWA mappings from a TSV file ('WORD','ABBREVIATION','LANGUAGES'),\n",
    "    filter for English entries, and return {word_lower: abbreviation}.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, sep='\\t', engine='python', dtype=str)\n",
    "    df = df.dropna(subset=['ABBREVIATION'])\n",
    "    df_en = df[df['LANGUAGES'].str.contains('English', case=False, na=False)]\n",
    "    return {row['WORD'].lower(): row['ABBREVIATION'] for _, row in df_en.iterrows()}\n",
    "\n",
    "def find_abbreviation(word: str, mapping: dict) -> str:\n",
    "    \"\"\"\n",
    "    Try to find an abbreviation for `word` in `mapping`:\n",
    "    1. Exact match\n",
    "    2. With trailing '-'\n",
    "    3. Iteratively truncate last char + '-'\n",
    "    If none found, return the original word.\n",
    "    \"\"\"\n",
    "    original = word\n",
    "    key = word.lower()\n",
    "    # 1) Exact\n",
    "    if key in mapping and mapping[key]:\n",
    "        return mapping[key]\n",
    "    # 2) Trailing dash\n",
    "    dash_key = f\"{key}-\"\n",
    "    if dash_key in mapping and mapping[dash_key]:\n",
    "        return mapping[dash_key]\n",
    "    # 3) Truncate + dash\n",
    "    truncated = key\n",
    "    while len(truncated) > 1:\n",
    "        truncated = truncated[:-1]\n",
    "        tr_key = f\"{truncated}-\"\n",
    "        if tr_key in mapping and mapping[tr_key]:\n",
    "            return mapping[tr_key]\n",
    "    # Fallback\n",
    "    return original\n",
    "\n",
    "def abbreviate_journal_title(title: str, mapping: dict) -> str:\n",
    "    \"\"\"\n",
    "    Abbreviate a full journal title by applying `find_abbreviation` to each word,\n",
    "    preserving punctuation.\n",
    "    \"\"\"\n",
    "    tokens = re.split(r'(\\W+)', title)\n",
    "    return ''.join(\n",
    "        find_abbreviation(tok, mapping) if re.match(r'\\w+', tok) else tok\n",
    "        for tok in tokens\n",
    "    )\n",
    "\n",
    "def abbreviate_journals_in_bib(input_bib_path: str, output_bib_path: str, ltwa_csv_path: str):\n",
    "    \"\"\"\n",
    "    Read a .bib file, abbreviate all `journal = {...}` fields using LTWA rules,\n",
    "    print each change (old vs new), and write out a new .bib file.\n",
    "    \"\"\"\n",
    "    # Load the mapping\n",
    "    mapping = load_ltwa_mapping(ltwa_csv_path)\n",
    "\n",
    "    # Read .bib content\n",
    "    with open(input_bib_path, 'r', encoding='utf-8') as f:\n",
    "        bib_text = f.read()\n",
    "\n",
    "    # Replace journal titles and print changes\n",
    "    journal_re = re.compile(r'(journal\\s*=\\s*\\{)([^}]+)(\\})', flags=re.IGNORECASE)\n",
    "\n",
    "    def repl(m):\n",
    "        prefix, full_title, suffix = m.groups()\n",
    "        abbr = abbreviate_journal_title(full_title, mapping)\n",
    "        if abbr != full_title:\n",
    "            print(f\"Replaced journal title:\\n  Old: {full_title}\\n  New: {abbr}\\n\")\n",
    "        return f\"{prefix}{abbr}{suffix}\"\n",
    "\n",
    "    updated = journal_re.sub(repl, bib_text)\n",
    "\n",
    "    # Write the result\n",
    "    with open(output_bib_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(updated)\n",
    "\n",
    "    print(f\"âœ… Abbreviated journals written to '{output_bib_path}'\")\n",
    "\n",
    "# Example usage:\n",
    "abbreviate_journals_in_bib(\n",
    "    input_bib_path='crossRef_cited_by_section.bib',\n",
    "    output_bib_path='abbrev_crossRef_cited_by_section.bib',\n",
    "    ltwa_csv_path='ltwa_current.csv'\n",
    ")\n",
    "\n",
    "def fix_encoding_artifacts(input_bib: str, output_bib: str):\n",
    "    \"\"\"\n",
    "    Reads input_bib, replaces:\n",
    "      â€¢ 'Ã¢â‚¬â€œ' â†’ '-'\n",
    "      â€¢ 'Ã¢â‚¬â„¢' â†’ \"'\"\n",
    "    and writes the cleaned text to output_bib.\n",
    "    \"\"\"\n",
    "    with open(input_bib, 'r', encoding='utf-8') as fin:\n",
    "        text = fin.read()\n",
    "\n",
    "    # Define all your replacement pairs here\n",
    "    replacements = {\n",
    "        'Ã¢â‚¬â€œ': '-',\n",
    "        'Ã¢â‚¬â„¢': \"'\",\n",
    "    }\n",
    "\n",
    "    # Apply each replacement\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "\n",
    "    with open(output_bib, 'w', encoding='utf-8') as fout:\n",
    "        fout.write(text)\n",
    "\n",
    "    print(f\"Cleaned file written to: {output_bib}\")\n",
    "\n",
    "\n",
    "# adjust filenames as needed:\n",
    "fix_encoding_artifacts(\n",
    "    input_bib='abbrev_crossRef_cited_by_section.bib',\n",
    "    output_bib='abbrev_crossRef_cited_by_section.bib')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
