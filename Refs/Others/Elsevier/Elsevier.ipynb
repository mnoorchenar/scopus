{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1e2077e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\0 Main Codes\\\\Refs\\\\Elsevier'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "# os.chdir('./Elsevier/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afe2ec9",
   "metadata": {},
   "source": [
    "# Order cite by section from latex file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3efad9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! 35 unique citations written to cited_by_section.bib\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def split_bib_by_section(tex_path: str, bib_path: str, out_bib_path: str):\n",
    "    r\"\"\"\n",
    "    Reads a LaTeX file (tex_path) and a BibTeX file (bib_path),\n",
    "    then writes a new BibTeX file (out_bib_path) containing only\n",
    "    those entries first cited in each \\section{…} of the .tex,\n",
    "    grouped and commented by section, **with no empty lines inside entries**.\n",
    "    \"\"\"\n",
    "    # 1. Read files\n",
    "    tex = open(tex_path,  encoding='utf-8').read()\n",
    "    bib = open(bib_path, encoding='utf-8').read()\n",
    "\n",
    "    # 2. Split into sections\n",
    "    sec_re = re.compile(r'\\\\section\\{([^}]+)\\}')\n",
    "    matches = list(sec_re.finditer(tex))\n",
    "    sections = []\n",
    "    if matches and matches[0].start() > 0:\n",
    "        sections.append(('Preamble', tex[:matches[0].start()]))\n",
    "    for i, m in enumerate(matches):\n",
    "        title = m.group(1).strip()\n",
    "        start = m.end()\n",
    "        end   = matches[i+1].start() if i+1 < len(matches) else len(tex)\n",
    "        sections.append((title, tex[start:end]))\n",
    "    if not matches:\n",
    "        sections = [('Document', tex)]\n",
    "\n",
    "    # 3. Gather first-time citations per section\n",
    "    cited = set()\n",
    "    section_order = []\n",
    "    cite_re = re.compile(r'\\\\cite\\w*\\{([^}]+)\\}')\n",
    "    for title, content in sections:\n",
    "        keys_in_sec = []\n",
    "        for g in cite_re.findall(content):\n",
    "            for key in g.split(','):\n",
    "                key = key.strip()\n",
    "                if key and key not in cited:\n",
    "                    cited.add(key)\n",
    "                    keys_in_sec.append(key)\n",
    "        if keys_in_sec:\n",
    "            section_order.append((title, keys_in_sec))\n",
    "\n",
    "    # 4. Load all Bib entries into a dict\n",
    "    bib_entries = {}\n",
    "    for entry in re.split(r'(?=@)', bib):\n",
    "        m = re.match(r'@\\w+\\{([^,]+),', entry)\n",
    "        if m:\n",
    "            bib_entries[m.group(1).strip()] = entry.strip()\n",
    "\n",
    "    # 5. Write filtered .bib (cleaning out blank lines within entries)\n",
    "    with open(out_bib_path, 'w', encoding='utf-8') as out:\n",
    "        for title, keys in section_order:\n",
    "            out.write(f\"% Section: {title}\\n\")\n",
    "            for k in keys:\n",
    "                raw_entry = bib_entries.get(k)\n",
    "                if raw_entry:\n",
    "                    # remove any entirely blank lines\n",
    "                    lines = raw_entry.splitlines()\n",
    "                    clean = [ln for ln in lines if ln.strip()]\n",
    "                    out.write(\"\\n\".join(clean) + \"\\n\\n\")\n",
    "                else:\n",
    "                    out.write(f\"% WARNING: no entry for '{k}'\\n\\n\")\n",
    "\n",
    "    print(f\"Done! {len(cited)} unique citations written to {out_bib_path}\")\n",
    "\n",
    "split_bib_by_section(\n",
    "    tex_path='main.tex',\n",
    "    bib_path='Refs.bib',\n",
    "    out_bib_path='cited_by_section.bib'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29efa00b",
   "metadata": {},
   "source": [
    "# Update by CrossREF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4a9315c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✗] No exact match for “Big data and health”, keeping original.\n",
      "[✗] No exact match for “Publicly available clinical BERT embeddings”, keeping original.\n",
      "[✗] No exact match for “Effect of a Predictive Model on Planned Surgical Duration Accuracy, Patient Wait Time, and Use of Presurgical Resources: A Randomized Clinical Trial”, keeping original.\n",
      "\n",
      "✅ Updated .bib written to: crossRef_cited_by_section.bib\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import string\n",
    "\n",
    "def normalize_title(t: str) -> str:\n",
    "    \"\"\"\n",
    "    Lowercase, strip out everything except letters, digits and spaces,\n",
    "    collapse multiple spaces into one.\n",
    "    \"\"\"\n",
    "    allowed = set(string.ascii_lowercase + string.digits + \" \")\n",
    "    t = t.lower()\n",
    "    t = \"\".join(ch for ch in t if ch in allowed)\n",
    "    return \" \".join(t.split())\n",
    "\n",
    "def update_bib_with_crossref_strict(input_bib_path: str, output_bib_path: str):\n",
    "    text = open(input_bib_path, encoding='utf-8').read()\n",
    "    # split by your Section headers\n",
    "    blocks = re.split(r'(?=%\\s*Section:)', text)\n",
    "    cache = {}\n",
    "    out = []\n",
    "\n",
    "    for block in blocks:\n",
    "        if not block.strip():\n",
    "            continue\n",
    "        # keep anything before the first \"% Section:\"\n",
    "        if not block.lstrip().startswith('% Section:'):\n",
    "            out.append(block)\n",
    "            continue\n",
    "\n",
    "        # Separate header line from the entries\n",
    "        header, *rest = block.splitlines()\n",
    "        header += \"\\n\"\n",
    "        content = \"\\n\".join(rest)\n",
    "        raw_entries = re.split(r'(?=@\\w+\\{)', content)\n",
    "        updated_entries = []\n",
    "\n",
    "        for raw in raw_entries:\n",
    "            entry = raw.strip()\n",
    "            if not entry or not entry.startswith('@'):\n",
    "                continue\n",
    "\n",
    "            # pull out entry type and your original key\n",
    "            m = re.match(r'@(\\w+)\\{([^,]+),', entry)\n",
    "            if not m:\n",
    "                updated_entries.append(entry)\n",
    "                continue\n",
    "            ent_type, orig_key = m.group(1), m.group(2)\n",
    "\n",
    "            # pull title / authors / year\n",
    "            t_m = re.search(r'title\\s*=\\s*\\{([^}]+)\\}', entry, re.I)\n",
    "            a_m = re.search(r'author\\s*=\\s*\\{([^}]+)\\}', entry, re.I)\n",
    "            y_m = re.search(r'year\\s*=\\s*\\{?(\\d{4})\\}?', entry, re.I)\n",
    "\n",
    "            if not t_m:\n",
    "                print(f\"[✗] No title for key '{orig_key}', keeping original.\")\n",
    "                updated_entries.append(entry)\n",
    "                continue\n",
    "\n",
    "            raw_title = t_m.group(1).strip()\n",
    "            norm_title = normalize_title(raw_title)\n",
    "\n",
    "            # if we've done this title before, reuse whatever we got\n",
    "            if norm_title in cache:\n",
    "                updated_entries.append(cache[norm_title])\n",
    "                continue\n",
    "\n",
    "            doi = None\n",
    "            # 1) title‐based search\n",
    "            try:\n",
    "                r1 = requests.get(\n",
    "                    'https://api.crossref.org/works',\n",
    "                    params={'query.title': raw_title, 'rows': 1}\n",
    "                )\n",
    "                if r1.ok:\n",
    "                    items = r1.json().get('message', {}).get('items', [])\n",
    "                    if items:\n",
    "                        cand = items[0]\n",
    "                        cand_title = cand.get('title', [''])[0]\n",
    "                        if normalize_title(cand_title) == norm_title:\n",
    "                            doi = cand.get('DOI')\n",
    "            except Exception as e:\n",
    "                print(f\"[✗] Title‐search error for '{raw_title}': {e}\")\n",
    "\n",
    "            # 2) fallback author+year, but **still** require title match\n",
    "            if not doi and a_m and y_m:\n",
    "                first_author = a_m.group(1).split(' and ')[0].split()[-1]\n",
    "                year = y_m.group(1)\n",
    "                try:\n",
    "                    r2 = requests.get(\n",
    "                        'https://api.crossref.org/works',\n",
    "                        params={\n",
    "                            'query.author': first_author,\n",
    "                            'filter': f'from-pub-date:{year},until-pub-date:{year}',\n",
    "                            'rows': 1\n",
    "                        }\n",
    "                    )\n",
    "                    if r2.ok:\n",
    "                        items2 = r2.json().get('message', {}).get('items', [])\n",
    "                        if items2:\n",
    "                            cand = items2[0]\n",
    "                            cand_title2 = cand.get('title', [''])[0]\n",
    "                            if normalize_title(cand_title2) == norm_title:\n",
    "                                doi = cand.get('DOI')\n",
    "                except Exception as e:\n",
    "                    print(f\"[✗] Fallback error for '{raw_title}': {e}\")\n",
    "\n",
    "            # 3) if we got a DOI, fetch fresh BibTeX and force your key\n",
    "            if doi:\n",
    "                try:\n",
    "                    r3 = requests.get(\n",
    "                        f'https://doi.org/{doi}',\n",
    "                        headers={'Accept': 'application/x-bibtex; charset=utf-8'}\n",
    "                    )\n",
    "                    if r3.ok:\n",
    "                        fresh = r3.text\n",
    "                        # swap in your original key\n",
    "                        fresh = re.sub(\n",
    "                            r'(@\\w+\\{)[^,]+,',\n",
    "                            fr'\\1{orig_key},',\n",
    "                            fresh, count=1\n",
    "                        )\n",
    "                        new_entry = fresh.strip()\n",
    "                    else:\n",
    "                        print(f\"[✗] Couldn’t fetch BibTeX for DOI {doi}, keeping original.\")\n",
    "                        new_entry = entry\n",
    "                except Exception as e:\n",
    "                    print(f\"[✗] Fetch error for DOI {doi}: {e}\")\n",
    "                    new_entry = entry\n",
    "            else:\n",
    "                # no valid DOI/title‐match\n",
    "                print(f\"[✗] No exact match for “{raw_title}”, keeping original.\")\n",
    "                new_entry = entry\n",
    "\n",
    "            cache[norm_title] = new_entry\n",
    "            updated_entries.append(new_entry)\n",
    "            time.sleep(random.uniform(10, 30))  # be polite\n",
    "\n",
    "        # reassemble this section\n",
    "        out.append(header)\n",
    "        for e in updated_entries:\n",
    "            out.append(e + \"\\n\\n\")\n",
    "\n",
    "    # write out\n",
    "    with open(output_bib_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\".join(out))\n",
    "\n",
    "    print(f\"\\n✅ Updated .bib written to: {output_bib_path}\")\n",
    "\n",
    "\n",
    "update_bib_with_crossref_strict(\n",
    "    input_bib_path='cited_by_section.bib',\n",
    "    output_bib_path='crossRef_cited_by_section.bib'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea3382b",
   "metadata": {},
   "source": [
    "# Abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f156c2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced journal title:\n",
      "  Old: JAMA Surgery\n",
      "  New: JAMA surg.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Mayo Clinic Proceedings: Innovations, Quality &amp; Outcomes\n",
      "  New: Mayo clin. proc.: Innovations, qual. &amp; Outcomes\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: European Journal of Operational Research\n",
      "  New: European j. of Operational res.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Computer Methods and Programs in Biomedicine\n",
      "  New: comput. Methods and Programs in biomed.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Journal of Combinatorial Optimization\n",
      "  New: j. of comb. optim.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: The Lancet Digital Health\n",
      "  New: The Lancet Digital heal.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Journal of Medical Systems\n",
      "  New: j. of med. syst.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Surgical Innovation\n",
      "  New: surg. Innovation\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Nature Communications\n",
      "  New: nat. commun.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: BMC Medical Research Methodology\n",
      "  New: BMC med. res. methodol.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: arXiv preprint arXiv:1904.03323\n",
      "  New: arXiv prepr. arXiv:1904.03323\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Journal of the American College of Surgeons\n",
      "  New: j. of the am. coll. of surg.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Computer Methods and Programs in Biomedicine\n",
      "  New: comput. Methods and Programs in biomed.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Journal of Medical Systems\n",
      "  New: j. of med. syst.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Otolaryngology–Head and Neck Surgery\n",
      "  New: Otolaryngology–Head and Neck surg.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Archives of Orthopaedic and Trauma Surgery\n",
      "  New: Archives of orthop. and Trauma surg.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Annals of Surgery\n",
      "  New: ann. of surg.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Journal of Medical Systems\n",
      "  New: j. of med. syst.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Surgical Endoscopy\n",
      "  New: surg. endosc.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: JAMA Surgery\n",
      "  New: JAMA surg.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Journal of Biomedical Informatics\n",
      "  New: j. of biomed. Informatics\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: BMC Medical Informatics and Decision Making\n",
      "  New: BMC med. Informatics and decis. mak.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Artificial Intelligence in Medicine\n",
      "  New: artif. intell. in Medicine\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Journal of the American Medical Informatics Association\n",
      "  New: j. of the am. med. Informatics assoc.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Journal of Healthcare Informatics Research\n",
      "  New: j. of healthc. Informatics res.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: JMIR Medical Informatics\n",
      "  New: JMIR med. Informatics\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Journal of the American Medical Informatics Association\n",
      "  New: j. of the am. med. Informatics assoc.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Computer Methods and Programs in Biomedicine\n",
      "  New: comput. Methods and Programs in biomed.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Surgery\n",
      "  New: surg.\n",
      "\n",
      "Replaced journal title:\n",
      "  Old: Arthroplasty Today\n",
      "  New: arthroplast. Today\n",
      "\n",
      "✅ Abbreviated journals written to 'abbrev_crossRef_cited_by_section.bib'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def load_ltwa_mapping(csv_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load LTWA mappings from a TSV file ('WORD','ABBREVIATION','LANGUAGES'),\n",
    "    filter for English entries, and return {word_lower: abbreviation}.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, sep='\\t', engine='python', dtype=str)\n",
    "    df = df.dropna(subset=['ABBREVIATION'])\n",
    "    df_en = df[df['LANGUAGES'].str.contains('English', case=False, na=False)]\n",
    "    return {row['WORD'].lower(): row['ABBREVIATION'] for _, row in df_en.iterrows()}\n",
    "\n",
    "def find_abbreviation(word: str, mapping: dict) -> str:\n",
    "    \"\"\"\n",
    "    Try to find an abbreviation for `word` in `mapping`:\n",
    "    1. Exact match\n",
    "    2. With trailing '-'\n",
    "    3. Iteratively truncate last char + '-'\n",
    "    If none found, return the original word.\n",
    "    \"\"\"\n",
    "    original = word\n",
    "    key = word.lower()\n",
    "    # 1) Exact\n",
    "    if key in mapping and mapping[key]:\n",
    "        return mapping[key]\n",
    "    # 2) Trailing dash\n",
    "    dash_key = f\"{key}-\"\n",
    "    if dash_key in mapping and mapping[dash_key]:\n",
    "        return mapping[dash_key]\n",
    "    # 3) Truncate + dash\n",
    "    truncated = key\n",
    "    while len(truncated) > 1:\n",
    "        truncated = truncated[:-1]\n",
    "        tr_key = f\"{truncated}-\"\n",
    "        if tr_key in mapping and mapping[tr_key]:\n",
    "            return mapping[tr_key]\n",
    "    # Fallback\n",
    "    return original\n",
    "\n",
    "def abbreviate_journal_title(title: str, mapping: dict) -> str:\n",
    "    \"\"\"\n",
    "    Abbreviate a full journal title by applying `find_abbreviation` to each word,\n",
    "    preserving punctuation.\n",
    "    \"\"\"\n",
    "    tokens = re.split(r'(\\W+)', title)\n",
    "    return ''.join(\n",
    "        find_abbreviation(tok, mapping) if re.match(r'\\w+', tok) else tok\n",
    "        for tok in tokens\n",
    "    )\n",
    "\n",
    "def abbreviate_journals_in_bib(input_bib_path: str, output_bib_path: str, ltwa_csv_path: str):\n",
    "    \"\"\"\n",
    "    Read a .bib file, abbreviate all `journal = {...}` fields using LTWA rules,\n",
    "    print each change (old vs new), and write out a new .bib file.\n",
    "    \"\"\"\n",
    "    # Load the mapping\n",
    "    mapping = load_ltwa_mapping(ltwa_csv_path)\n",
    "\n",
    "    # Read .bib content\n",
    "    with open(input_bib_path, 'r', encoding='utf-8') as f:\n",
    "        bib_text = f.read()\n",
    "\n",
    "    # Replace journal titles and print changes\n",
    "    journal_re = re.compile(r'(journal\\s*=\\s*\\{)([^}]+)(\\})', flags=re.IGNORECASE)\n",
    "\n",
    "    def repl(m):\n",
    "        prefix, full_title, suffix = m.groups()\n",
    "        abbr = abbreviate_journal_title(full_title, mapping)\n",
    "        if abbr != full_title:\n",
    "            print(f\"Replaced journal title:\\n  Old: {full_title}\\n  New: {abbr}\\n\")\n",
    "        return f\"{prefix}{abbr}{suffix}\"\n",
    "\n",
    "    updated = journal_re.sub(repl, bib_text)\n",
    "\n",
    "    # Write the result\n",
    "    with open(output_bib_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(updated)\n",
    "\n",
    "    print(f\"✅ Abbreviated journals written to '{output_bib_path}'\")\n",
    "\n",
    "# Example usage:\n",
    "abbreviate_journals_in_bib(\n",
    "    input_bib_path='crossRef_cited_by_section.bib',\n",
    "    output_bib_path='abbrev_crossRef_cited_by_section.bib',\n",
    "    ltwa_csv_path='ltwa_current.csv'\n",
    ")\n",
    "\n",
    "def fix_encoding_artifacts(input_bib: str, output_bib: str):\n",
    "    \"\"\"\n",
    "    Reads input_bib, replaces:\n",
    "      • 'â€“' → '-'\n",
    "      • 'â€™' → \"'\"\n",
    "    and writes the cleaned text to output_bib.\n",
    "    \"\"\"\n",
    "    with open(input_bib, 'r', encoding='utf-8') as fin:\n",
    "        text = fin.read()\n",
    "\n",
    "    # Define all your replacement pairs here\n",
    "    replacements = {\n",
    "        'â€“': '-',\n",
    "        'â€™': \"'\",\n",
    "    }\n",
    "\n",
    "    # Apply each replacement\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "\n",
    "    with open(output_bib, 'w', encoding='utf-8') as fout:\n",
    "        fout.write(text)\n",
    "\n",
    "    print(f\"Cleaned file written to: {output_bib}\")\n",
    "\n",
    "\n",
    "# adjust filenames as needed:\n",
    "fix_encoding_artifacts(\n",
    "    input_bib='abbrev_crossRef_cited_by_section.bib',\n",
    "    output_bib='abbrev_crossRef_cited_by_section.bib')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
